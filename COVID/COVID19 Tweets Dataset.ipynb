{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emerging Topic Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the study is to develop a NLP model to help authorities to deal with crises. The idea is to select dataset containing emerging topic. Considering those requirements, it has been decided to work on #COVID19 Twitter datasets.\n",
    "The next step is to choose a dataset that meet the NLP-model requirements among tons of dataset available on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dataset selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 - CODA-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A study was conducted in April by scientists creating a 10,000 tweets dataset specifically to train NLP model. The study is available on this link: https://arxiv.org/abs/2005.02367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coda_paper_id</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>coda_data_split</th>\n",
       "      <th>subset</th>\n",
       "      <th>paragraph_num</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>segment_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th>coda_has_expert_labels</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>006be5ba67759a525ae8e211f43bd8e4429a64f0</td>\n",
       "      <td>test</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>235</td>\n",
       "      <td>False</td>\n",
       "      <td>Human astroviruses: in silico analysis of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6a0da82f10bac49659556d074a71136e794ae45d</td>\n",
       "      <td>test</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>204</td>\n",
       "      <td>True</td>\n",
       "      <td>Public health in practice: the three domains o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1b822763c007bb77798727490e95ee2bec342494</td>\n",
       "      <td>test</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>322</td>\n",
       "      <td>False</td>\n",
       "      <td>Preparedness of institutions around the world ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>97ebcdaaa4b9dd8f174eb3aa4137231b5af413fb</td>\n",
       "      <td>test</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>Effect of the Ebola-virus-disease epidemic on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>83303635687128ac583152565ba1d7e29540f2af</td>\n",
       "      <td>test</td>\n",
       "      <td>noncomm_use_subset</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>213</td>\n",
       "      <td>False</td>\n",
       "      <td>Health care-associated infections -an overview</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coda_paper_id                                  paper_id coda_data_split  \\\n",
       "0              1  006be5ba67759a525ae8e211f43bd8e4429a64f0            test   \n",
       "1              2  6a0da82f10bac49659556d074a71136e794ae45d            test   \n",
       "2              3  1b822763c007bb77798727490e95ee2bec342494            test   \n",
       "3              4  97ebcdaaa4b9dd8f174eb3aa4137231b5af413fb            test   \n",
       "4              5  83303635687128ac583152565ba1d7e29540f2af            test   \n",
       "\n",
       "               subset  paragraph_num  sentence_num  segment_num  token_num  \\\n",
       "0      custom_license              1             9           16        235   \n",
       "1      custom_license              2             8           13        204   \n",
       "2      custom_license              1            10           15        322   \n",
       "3      custom_license              1             3            4         73   \n",
       "4  noncomm_use_subset              2             6           10        213   \n",
       "\n",
       "   coda_has_expert_labels                                              title  \n",
       "0                   False  Human astroviruses: in silico analysis of the ...  \n",
       "1                    True  Public health in practice: the three domains o...  \n",
       "2                   False  Preparedness of institutions around the world ...  \n",
       "3                   False  Effect of the Ebola-virus-disease epidemic on ...  \n",
       "4                   False     Health care-associated infections -an overview  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"/home/urendil/Documents/01-ENSTA/Cours/PRE/COVID/CODA-19/data/CODA19_v1_20200504/human_label/coda_metadata.csv\",encoding='unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic useful libraries\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Particular library from Nucleus for this notebook\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "    \n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'nucleus.sumup.ai:5000'\n",
    "configuration.api_key['x-api-key'] = 'zGtJTrTa4izSMMdssWpOeg'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the connection and dataset available on the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 datasets in the database:\n",
      "     CODA19\n",
      "     COVID19_geolocation\n",
      "     Drouet_letters_cleaned\n",
      "     ASRS3\n",
      "     ASRS1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "\n",
    "list_datasets = api_response.result\n",
    "\n",
    "print(len(list_datasets), 'datasets in the database:')\n",
    "for ds in list_datasets:\n",
    "    print('    ', ds.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset: 'time','title' and 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./CODA-19/data/CODA19_v1_20200504/human_label/coda_metadata.csv','r',encoding='UTF8',errors='ignore') as r:\n",
    "    reader = csv.DictReader(r)\n",
    "    # Creating an output file to write the preprocessed dataset in\n",
    "    with open('./CODA-19/data/CODA19_v1_20200504/human_label/20200525_coda19.csv','w') as w: \n",
    "        fieldnames=['time','title','content']\n",
    "        writer = csv.DictWriter(w,fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        # For each line in the input file, write in the output file specifically the row 'coda_paper_id','subset' and 'title' as 'time', 'title' and 'content' \n",
    "        for row in reader:\n",
    "            line=[row['coda_paper_id'],row['subset'],row['title']]\n",
    "            writer.writerow({'time':line[0],'title':line[1],'content':line[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Start polling job status of 2722780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-02 09:53:20,993 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /jobs?id=2722780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Job 2722780 completed.\n",
      "1 JSON records( 28090368 bytes) appended to CODA19\n"
     ]
    }
   ],
   "source": [
    "csv_file = './CODA-19/data/CODA19_v1_20200504/human_label/20200525_coda19.csv'\n",
    "dataset = 'CODA19'\n",
    "with open(csv_file,'r',encoding='UTF8',errors='ignore') as r:\n",
    "    reader = csv.DictReader(r)\n",
    "    for row in reader:\n",
    "        json_props = nucleus_helper.upload_jsons(api_instance, dataset, reader, processes=1)\n",
    "        #json_props = nucleus_api.Appendjsonparams(dataset=dataset, language='english', document={ 'time':row['time'], 'title':row['title'], 'content':row['content']})\n",
    "        #api_response = api_instance.post_append_json_to_dataset(json_props)\n",
    "    total_size = 0\n",
    "    total_jsons = 0\n",
    "    for jp in json_props:\n",
    "        total_size += jp.size\n",
    "        total_jsons += 1\n",
    "    print(total_jsons,'JSON records(', total_size, 'bytes) appended to', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 keywords:\n",
      "    Keywords: respiratory syndrome;acute respiratory;middle east;east respiratory;severe acute;syndrome coronavirus;syndrome outbreak;management middle\n",
      "    Keyword weights: 0.1297;0.15;0.2679;0.1192;0.1346;0.1329;0.0379;0.0278\n",
      "    Strength: 0.2518\n",
      "---------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2b8c78babee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#print('    Document exposures:', doc_topic_exposure_sel_str)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords_weight_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "#custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "custom_stop_words=''\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "S=[]\n",
    "K=[]\n",
    "W=[]\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection,\n",
    "                                time_period=time_period)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "\n",
    "key = 0\n",
    "\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    #print('    Document IDs:', doc_id_sel_str)\n",
    "    #print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')\n",
    "    s.append(res.strength)\n",
    "    k.append(res.keywords)\n",
    "    w.append(keywords_weight_str)\n",
    "    \n",
    "S.append(s)\n",
    "K.append(k)\n",
    "W.append(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create csv files to store the keywords, strenghts and weights from the API results.\n",
    "We may need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=len(K)\n",
    "df_keywords = pd.DataFrame({}, index = [k for k in range (1, len(K[0]))])\n",
    "for i in range (m):\n",
    "    df_keywords[K[i][0]] = [K[i][j] for j in range (1, len(K[i]))]\n",
    "df_keywords.to_csv('keywords.csv', sep=';', header=True)\n",
    "\n",
    "n=len(S)\n",
    "df_strength = pd.DataFrame({}, index = [k for k in range (1, len(S[0]))])\n",
    "for i in range (n):\n",
    "    df_strength[S[i][0]] = [S[i][j] for j in range (1, len(S[i]))]\n",
    "df_strength.to_csv('strength.csv', sep=';', header=True)\n",
    "\n",
    "p=len(W)\n",
    "df_strength = pd.DataFrame({}, index = [k for k in range (1, len(W[0]))])\n",
    "for i in range (p):\n",
    "    df_strength[W[i][0]] = [W[i][j] for j in range (1, len(W[i]))]\n",
    "df_strength.to_csv('weights.csv', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_list(L):\n",
    "    MyFile=open('Keywords.txt','w')\n",
    "    n=len(L)\n",
    "    for i in range (n):\n",
    "        T=L[i]\n",
    "        p=len(T)\n",
    "        for j in range(p):\n",
    "            MyFile.write(T[j]+'\\n')\n",
    "    MyFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list(S)\n",
    "store_list(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content is stored in a file that will be uploaded into the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./CODA-19/data/CODA19_v1_20200504/human_label/coda_metadata.csv','r',encoding='UTF8',errors='ignore') as r:\n",
    "    reader = csv.DictReader(r)\n",
    "    with open('coda19.txt','w') as w: \n",
    "        fieldnames=['content']\n",
    "        writer = csv.DictWriter(w,fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        # For each line in the input file, write in the output file specifically the row 'coda_paper_id','subset' and 'title' as 'time', 'title' and 'content' \n",
    "        for row in reader:\n",
    "            line=[row['title']]\n",
    "            writer.writerow({'content':line[0]})         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, the content is required to be computed to chop out/off single words or redundancy.\n",
    "\n",
    "After that, the words from the content will be vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=open('coda19.txt','r')\n",
    "document=t.readlines()\n",
    "\n",
    "# api_response.result\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#document = document.split()\n",
    "\n",
    "document = [stemmer.lemmatize(word) for word in document]\n",
    "document = ' '.join(document)\n",
    "\n",
    "documents.append(document)\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(document).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  $\\textit{Relevance of the dataset}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the choice of the dataset relevent? Is a different dataset changed the output of the model? How close? Distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 - COVID19 TweetsID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I tried to download the following dataset: https://github.com/echen102/COVID-19-TweetIDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_json(\"/home/urendil/Documents/01-ENSTA/Cours/PRE/COVID/COVID-19-TweetIDs/2020-01/coronavirus-tweet-id-2020-01-21-23.jsonl\",encoding='unicode_escape')\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 - Kaggle geolocation COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from plotly.offline import init_notebook_mode, iplot \n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import pycountry\n",
    "py.init_notebook_mode(connected=True)\n",
    "import folium \n",
    "from folium import plugins\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "\n",
    "# Graphics in retina format \n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "\n",
    "# Increase the default plot size and set the color scheme\n",
    "plt.rcParams['figure.figsize'] = 8, 5\n",
    "#plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Disable warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reading the dataset\n",
    "data = pd.read_csv(\"Kaggle/COVID-19_geo_timeseries_ver_0311.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.data_source=='jhu']\n",
    "# Convert Last Update column to datetime64 format\n",
    "data['update_time'] = pd.to_datetime(data['update_time'])\n",
    "print(data['update_time'].dtype)\n",
    "# Extract date from the timestamp\n",
    "data['update_date'] = data['update_time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from fbprophet import Prophet\n",
    "import pycountry\n",
    "import plotly.express as px\n",
    "\n",
    "# Import data from John Hopkins University\n",
    "df = data[data.data_source=='jhu']\n",
    "df_agg = df.groupby('update_date').agg({'confirmed_cases':'sum','deaths':'sum','recovered':'sum'}).reset_index()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=df_agg['update_date'],\n",
    "                y=df_agg['confirmed_cases'],\n",
    "                name='Confirmed',\n",
    "                marker_color='blue'\n",
    "                ))\n",
    "fig.add_trace(go.Bar(x=df_agg['update_date'],\n",
    "                y=df_agg['deaths'],\n",
    "                name='Deaths',\n",
    "                marker_color='Red'\n",
    "                ))\n",
    "fig.add_trace(go.Bar(x=df_agg['update_date'],\n",
    "                y=df_agg['recovered'],\n",
    "                name='Recovered',\n",
    "                marker_color='Green'\n",
    "                ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Worldwide Corona Virus Cases - Confirmed, Deaths, Recovered',\n",
    "    xaxis_tickfont_size=14,\n",
    "    yaxis=dict(\n",
    "        title='Number of Cases',\n",
    "        titlefont_size=16,\n",
    "        tickfont_size=14,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1.0,\n",
    "        bgcolor='rgba(255, 255, 255, 0)',\n",
    "        bordercolor='rgba(255, 255, 255, 0)'\n",
    "    ),\n",
    "    barmode='group',\n",
    "    bargap=0.15, # gap between bars of adjacent location coordinates.\n",
    "    bargroupgap=0.1 # gap between bars of the same location coordinate.\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries affected\n",
    "countries = data[(data['country']!='Others') & (data['country']!='Undisclosed')]['country'].unique().tolist()\n",
    "# Use this print trick to get more readable list output\n",
    "print(*countries, sep = \"\\n\")\n",
    "print(\"\\nTotal countries affected by COVID-19: \",len(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest timestamp\n",
    "latest_date = data['update_time'].max()\n",
    "# extract year, month, day from the latest timestamp so we can use it just report the latest data\n",
    "year = latest_date.year\n",
    "month = latest_date.month\n",
    "# adjust for timezone\n",
    "day = latest_date.day - 1\n",
    "\n",
    "# Filter to only include the latest day data\n",
    "from datetime import date\n",
    "data_latest = data[data['update_time'] > pd.Timestamp(date(year,month,day))]\n",
    "data_latest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with total no of confirmed cases for every country as of the latest available date\n",
    "affected_country_latest = data_latest.groupby(['country','country_code','region','latitude','longitude','country_flag']).agg({'update_time': np.max}).reset_index()\n",
    "key = ['country','country_code','region','latitude','longitude','country_flag','update_time']\n",
    "global_cases = pd.merge(data_latest, affected_country_latest, how='inner', on=key).drop_duplicates().groupby(key).max().sort_values(by=['confirmed_cases'],ascending=False).reset_index()\n",
    "global_cases.index+=1\n",
    "global_cases_columns = global_cases.columns.tolist()\n",
    "global_cases_columns.remove('update_time')\n",
    "global_cases = global_cases[global_cases_columns]\n",
    "global_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_url = './Kaggle/world-countries.json'\n",
    "world_geo = shape_url\n",
    "\n",
    "m = folium.Map(location=[35.86166,104.195397], zoom_start=3,tiles='Stamen Toner')\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=world_geo,\n",
    "    name='choropleth',\n",
    "    data=global_cases,\n",
    "    columns=['country', 'confirmed_cases'],\n",
    "    key_on='feature.properties.name',\n",
    "    fill_color='OrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Number of Confirmed Cases'\n",
    ").add_to(m)\n",
    "\n",
    "for lat, lon, value, name in zip(global_cases['latitude'], global_cases['longitude'], global_cases['confirmed_cases'], global_cases['country']):\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=10,\n",
    "        popup = ('<strong>Country</strong>: ' + str(name).capitalize() + '<br>'\n",
    "                 '<strong>Confirmed Cases</strong>: ' + str(value) + '<br>'),        \n",
    "        color='orange',\n",
    "        fill=True,\n",
    "        fill_color='orange',\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with total no of confirmed cases for every country for all available dates\n",
    "key1 = ['country','country_code','region','latitude','longitude','country_flag','update_date']\n",
    "key2 = ['country','country_code','region','latitude','longitude','country_flag','update_date','confirmed_cases','deaths','recovered']\n",
    "key3 = ['country','country_code','region','latitude','longitude','country_flag']\n",
    "df_full = data[data.data_source == 'jhu'][key2].drop_duplicates().groupby(key1).max().reset_index()\n",
    "# df_full = data[key2].drop_duplicates().groupby(key1).max().sort_values(by=['country','update_date']).groupby(key3).cumsum().sort_values(by=['confirmed_cases','update_date'],ascending=[False,False]).reset_index()\n",
    "# df_full = df_full.groupby(key1).agg({'confirmed_cases':np.cumsum, 'deaths':np.cumsum ,'recovered':np.cumsum}).reset_index()\n",
    "df_full[['confirmed_cases','deaths','recovered']] = df_full[['confirmed_cases','deaths','recovered']].fillna(0)\n",
    "df_full['log_confirmed_cases'] = np.log(df_full['confirmed_cases'])\n",
    "df_full.sort_values(by=['confirmed_cases','update_date'],ascending=[False,False]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "scl = [[0.0, '#e7e1ef'],[0.2, '#d4b9da'],[0.4, '#c994c7'], \n",
    "       [0.6, '#df65b0'],[0.8, '#dd1c77'],[1.0, '#980043']] # reds\n",
    "\n",
    "data_slider = []\n",
    "all_dates = df_full['update_date'].sort_values().unique()\n",
    "for m,d in zip(pd.DatetimeIndex(all_dates).month,pd.DatetimeIndex(all_dates).day):\n",
    "    df_selected = df_full[(pd.DatetimeIndex(df_full['update_date']).month==m) & (pd.DatetimeIndex(df_full['update_date']).day==d)]\n",
    "    df_selected['text'] =   'Date: '+ df_selected['update_date'].astype(str) \\\n",
    "                            + '<br>' + 'Confirmed Cases: ' + df_selected['confirmed_cases'].astype(str) \\\n",
    "                            + '<br>' + 'Deaths: '+ df_selected['deaths'].astype(str) \\\n",
    "                            + '<br>' + 'Recovered: '+ df_selected['recovered'].astype(str)\n",
    "    data_one_day = dict(\n",
    "        type='choropleth',\n",
    "        colorscale = scl,\n",
    "        autocolorscale=False,\n",
    "        locations = df_selected['country'].tolist(),\n",
    "        z = df_selected['log_confirmed_cases'].tolist(),\n",
    "        locationmode = 'country names',\n",
    "        text = df_selected['text'],\n",
    "        colorbar_title = 'Confirmed Cases (Logarithm)'\n",
    "    )\n",
    "    data_slider.append(data_one_day)\n",
    "\n",
    "steps = []\n",
    "for i in range(len(data_slider)):\n",
    "    step = dict(method='restyle',\n",
    "                args=['visible', [False] * len(data_slider)],\n",
    "                label=(datetime.strptime('2020-01-21','%Y-%m-%d') + timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "               )\n",
    "    step['args'][1][i] = True\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(active=0, pad={\"t\": 1}, steps=steps)]  \n",
    "\n",
    "lyt = dict(\n",
    "    geo=dict(scope='world'), \n",
    "    sliders=sliders, \n",
    "    title_text = 'COVID-19 Trend Analysis (World)' + '<br>' + '(Hover for breakdown)'\n",
    ")\n",
    "fig = dict(data=data_slider, layout=lyt)\n",
    "plotly.offline.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 - COVID-19 Tweets dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried this one: https://github.com/lopezbec/COVID19_Tweets_Dataset\n",
    "It would be interesting to work on this dataset but the tweets are unreadable and they first have to be computed out (twarc and hydrate). This process takes time.\n",
    "To cope with those points, 2.000 tweets/day are randomly picked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc\n",
    "import os\n",
    "\n",
    "consumer_key = \"6ojMMS2mWsHgHA9aDwrMk9WG5\"\n",
    "consumer_secret = \"KJB6JmTaFKpHwOSAdpIpYibf8SHe6r2qzzFsOunSqfToO0QXS0\"\n",
    "access_token = \"968116193663049729-AcELLvOcLGYyhU2MPB1C0TZBLRstqrV\"\n",
    "access_token_secret = \"2VvILAdL5F5keEe8hiJuKfuuizUQoQ15nJNJdPgSv3fmy\"\n",
    "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A colab code (https://colab.research.google.com/drive/1reOEk79e8XR5etY6QZBGTkS769X5lEyk) is used to generate a human-readable dataset. But the process using twarc and hydrate is greedy regarding that a day is about 1.5 million tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check Keywords to Hydrate { run: \"auto\" }\n",
    "coronavirus = True #@param {type:\"boolean\"}\n",
    "virus = False #@param {type:\"boolean\"}\n",
    "covid = True #@param {type:\"boolean\"}\n",
    "ncov19 = False #@param {type:\"boolean\"}\n",
    "ncov2019 = False #@param {type:\"boolean\"}\n",
    "keyword_dict = {\"coronavirus\": coronavirus, \"virus\": virus, \"covid\": covid, \"ncov19\": ncov19, \"ncov2019\": ncov2019}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate tweets from january $1^{st}$ to may $15^{th}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.114 thousand unique tweets.\n"
     ]
    }
   ],
   "source": [
    "#@title Enter range of dates to Hydrate { run: \"auto\" }\n",
    "start_date = '2020-03-01' #@param {type:\"date\"}\n",
    "end_date = '2020-05-01' #@param {type:\"date\"}\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "files = []\n",
    "covid_loc = \"COVID19_Tweets_Dataset\"\n",
    "# Looks at each volder\n",
    "for folder in os.listdir(covid_loc):\n",
    "    foldername = os.fsdecode(folder)\n",
    "    # The folder name is a keyword. We continue for keywords selected above\n",
    "    if keyword_dict.get(foldername.split()[0].lower()) == True:\n",
    "        folderpath = os.path.join(covid_loc, foldername)\n",
    "        # Each file is of the format [keyword]_yyyy_mm_dd.txt\n",
    "        for file in os.listdir(folderpath):\n",
    "            filename = os.fsdecode(file)\n",
    "            date = filename[filename.index(\"_\")+1:filename.index(\".\")]\n",
    "\n",
    "            # If the date is within the required range, it is added to the\n",
    "            # list of files to read.\n",
    "            if (dt.datetime.strptime(start_date, \"%Y-%m-%d\").date() \n",
    "            <= dt.datetime.strptime(date, '%Y_%m_%d').date()\n",
    "             <= dt.datetime.strptime(end_date, \"%Y-%m-%d\").date()):\n",
    "                files.append(os.path.join(folderpath, filename))\n",
    "# The final list is read, and each of the individual IDs is stored in a collective\n",
    "# set of IDs. Duplicates are removed.\n",
    "ids = []\n",
    "for filename in files:\n",
    "    with open(filename) as f:\n",
    "        # The files are of the format: [id1,id2,id3,...,idn]\n",
    "        # Remove the brackets and split on commas\n",
    "        ids1 = []\n",
    "        for i in f.readline().strip('][').replace(\" \", \"\").split(\",\"):\n",
    "            ids1.append(i)\n",
    "        ids.append(random.choices(ids1,k=nb_tweets_max))   \n",
    "# Number of tweets read.\n",
    "print(round((len(ids)/1000), 3), \"thousand unique tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Enter ID output file {run: \"auto\"}\n",
    "final_tweet_ids_filename = \"final_ids.txt\" #@param {type: \"string\"}\n",
    "# The set of IDs is stored in this file.\n",
    "with open(final_tweet_ids_filename, \"w+\") as f:\n",
    "    for id in ids:\n",
    "        f.write('%s\\n' % id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set up Directory { run: \"auto\"}\n",
    "final_tweet_ids_filename = \"final_ids.txt\" #@param {type: \"string\"}\n",
    "output_filename = \"output.csv\" #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the high number of tweets per day (1.5 million), they can not all be hydrated. Let randomly pick up 2000 tweets per day.\n",
    "\n",
    "$\\textbf{DO NOT WORK}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1236443419335176192'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-03ce1da47af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# These tweets have already been hydrated. So remove them from ids_to_hydrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mhydrated_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mids_to_hydrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id_str\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total IDs: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", IDs to hydrate: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_to_hydrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hydrated: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhydrated_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1236443419335176192'"
     ]
    }
   ],
   "source": [
    "import jsonlines, json\n",
    "# Stores hydrated tweets here as jsonl objects\n",
    "# Contains one json object per line\n",
    "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
    "ids = []\n",
    "\n",
    "with open(final_tweet_ids_filename, \"r\") as ids_file:\n",
    "    ids = ids_file.read().split()\n",
    "    hydrated_tweets = []\n",
    "    ids_to_hydrate = set(ids)\n",
    "\n",
    "# Looks at the output file for already hydrated tweets\n",
    "if os.path.isfile(output_json_filename):\n",
    "    with jsonlines.open(output_json_filename, \"r\") as reader:\n",
    "        for i in reader.iter(type=dict, skip_invalid=True):\n",
    "            # These tweets have already been hydrated. So remove them from ids_to_hydrate\n",
    "            hydrated_tweets.append(i)\n",
    "            ids_to_hydrate.remove(i[\"id_str\"])\n",
    "print(\"Total IDs: \" + str(len(ids)) + \", IDs to hydrate: \" + str(len(ids_to_hydrate)))\n",
    "print(\"Hydrated: \" + str(len(hydrated_tweets)))\n",
    "\n",
    "count = len(hydrated_tweets)\n",
    "start_index = count # The index from where tweets haven't been saved to the output_json_file\n",
    "# Stores hydrated tweets to output_json_file every num_save iterations.\n",
    "num_save  = 1000\n",
    "\n",
    "\n",
    "# Now, use twarc and start hydrating\n",
    "\n",
    "for tweet in t.hydrate(ids_to_hydrate):\n",
    "    hydrated_tweets.append(tweet)\n",
    "    count += 1\n",
    "    # If num_save iterations have passed,\n",
    "    if (count % num_save) == 0:\n",
    "        # Open the output file\n",
    "        # NOTE: Even if the code stops during IO, only tweets from the current iteration are lost.\n",
    "        # Older tweets are preserved as the file is written in append mode.\n",
    "        with jsonlines.open(output_json_filename, \"a\") as writer:\n",
    "            print(\"Started IO\")\n",
    "            # Now write the tweets from start_index. The other tweets don't have to be written\n",
    "            # as they were already written in a previous iteration or run.\n",
    "            for hydrated_tweet in hydrated_tweets[start_index:]:\n",
    "                writer.write(hydrated_tweet)\n",
    "            print(\"Finished IO\")\n",
    "        print(\"Saved \" + str(count) + \" hydrated tweets.\")\n",
    "        # Now, since everything has been written. Reset start_index\n",
    "        start_index = count\n",
    "\n",
    "\n",
    "# There might be tweets unwritten in the last iteration if the count is not a multiple of num_tweets.\n",
    "# In that case, just write out the remainder of tweets.\n",
    "if count != start_index:\n",
    "    print(\"Here with start_index\", start_index)\n",
    "    with jsonlines.open(output_json_filename, \"a\") as writer:\n",
    "        for hydrated_tweet in hydrated_tweets[start_index:]:\n",
    "           writer.write(hydrated_tweet)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert jsonl to csv\n",
    "import csv, jsonlines\n",
    "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
    "# These are the column name that are selected to be stored in the csv\n",
    "keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
    "          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
    "          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
    "          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
    "          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
    "          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
    "          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
    "          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
    "          \"quoted_status_permalink\"]\n",
    "hydrated_tweets = []\n",
    "# Reads the current tweets\n",
    "with jsonlines.open(output_json_filename, \"r\") as reader:\n",
    "    for i in reader.iter(type=dict, skip_invalid=True):\n",
    "        hydrated_tweets.append(i)\n",
    "# Writes them out\n",
    "with  open(output_filename, \"w+\") as output_file:\n",
    "    d = csv.DictWriter(output_file, keyset)\n",
    "    d.writeheader()\n",
    "    d.writerows(hydrated_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-GeoCOVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A NLP Geolocalized dataset provided by : https://crisisnlp.qcri.org/covid19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-14a85f10c874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/urendil/Documents/01-ENSTA/Cours/PRE/COVID/GeoCOVID/geo_2020-02-01.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1089\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m             )\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"/home/urendil/Documents/01-ENSTA/Cours/PRE/COVID/GeoCOVID/geo_2020-02-01.json\",encoding='unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
